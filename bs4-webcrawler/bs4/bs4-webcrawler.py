import requests
from bs4 import BeautifulSoup
import urllib.parse
import time
from collections import Counter
import re

# 제외할 단어 리스트
# 저출산일 경우의 단어 필터링
EXCLUDE_WORDS = [ '은', '는', '이', '가', '께서', '이란', '란', '이니까', '는가','이도', '도', '라도', '밖에', '말고', '의', '의가', '이어', '로서','이면서', '이기 때문에', '와', '과', '부터', '에', '가지고', '로','처럼', '같이', '의해', '으로', '하여', '기 때문에', '뿐이다', '보다','하니', '라는', '냐고', '게', '때문에', '이라면', '이로', '이라서','든지', '또는', '뿐만 아니라', '까지', '로써', '대해서', '이래서', '이자','이기도 하고', '가치', '에도', '하니까', '하더라도', '관해서', '향해서','이해', '하고서', '와 함께', '과 더불어', '이후', '또한', '의하', '에 대한','하여서', '마치', '하기에', '만큼', '따라서', '같은', '지','해도', '하게', '이로서', '이처럼', '한테', '언제', '라고도', '다시', '과도','와도', '그러나', '자체', '단', '조차', '대해', '저', '한테서', '예를 들어', '뿐', '정도', '게다가', '이야', '없이', '더라도', '하기 위해', '자', '씩', '여', '그것', '상황에서','달리', '안에', '을','를', '에 의해서','의해서', '지라도', '한편', '관련해서','각','각각','위해', '말하자면', '따라', '비해', '반해', '이후에', '따르다','다만', '의 경우', '및', '조차도', '에 비해', '에 따라', '왜냐하면', '미래','라든지', '물론', '여기', '무엇', '어디', '어떤', '그렇게', '이렇게','왜', '어떻게',

'가능한','명을','되는','부족','점차','다양한','원인이','것이','특정','되었다','에서는','수','찾기','어렵다','주된','유배우','바','있다','경우','이러한','대부분','과거','대한','영국','년','아닌','주로','것을','세기','에','년부터','년까지','기간','동안','오히려','그','훨씬','여러','가진','가운데','높은','미국','만','때','따르면','사는','것으로','나타났다','이용한','년의','해당','지역','수가','이는','하는','한국과','국가의','중요한','내','간','등의','더','있는','반면','일부','현상이','영향','아시아','국가에서','년대','인해','크게','혹은','결과를','약','미치는','인한','영향을','자료','배','등으로','실제로','큰','반대로','상대적으로','이에','유럽','명','그리고','여전히','있으나','다른','낮은','등에','한','수준으로','이를','명까지','것이라고','하지만','들어','가장','후','다시','수준을','등은','수준이','기준으로','매우','것은','수준의','가정을','한다는','보고','대신','사용할','등이','이런','당시','것이다','불구하고','전체','가지고','통해','이나','수준','있으며','경향이','이미','많은','계속','보인다','특히','사람들이','명이','있어','부담을','주요','명에서','명으로','전','주','총','중','있고','년간','일','인','될','월','원','대상으로','있는데','세','로','있도록','등을','프랑스는','낳는','있다는','프랑스','스웨덴','잘','한다고','새로운','이민','독일','데','대해서는','하고','모든','있게','모두','결과','한국은','보면','하지','않은','일반','기준','명의','되면','없는','추가','지속적으로','일을','부모가','확대','할','최대','학교','년에','년에는','일가정','가능','위한','지속','많이','볼','구매','시','이상','이상의','따른','이다','있지만','많다','낳지','또','세계','되고','서비스','공식','어느','국가가','중국','두','이유는','때문이다','있기','않을','것이라는','인당','떨어졌다','정부가','지난','기존의','더욱','역대','명에','명대로','지난해','그런데','올해','분기','명대','출처','하나','안','우리','좋은','세계에서','메뉴','편집','안내','문서','문서의','내용은''입니다','문서를','참고하십시오','목차','관련','보기','러시아','각종','상승','보는','사람','변화','없을','못한','높다','있습니다','자세한','등록','이동','링크','토론','제차','대전','싱가포르','나','다','북한','내용','첫','필요한','포함한','이용','개인정보처리방침','소개','통계','모바일','최근','검색','로그인','개','다운로드','항목','판''되지','일에','확인함','관한','중심으로','한국은행','것','아니라','다음','대','않았다','세대가','이용할','있을','직접','수정','합니다','번','문단을','부분을','본','한다','함께','실제','즉','다음과','않으면','그래서','해도','정도로','심지어','사람들은','거의','해서','않는','경우도','사람이','너무','상황이다','그런','동시에','제대로','편이다','비율','없다','삶의','대부분의','사실','못','지금','것이고','이유로','줄','이들','어려운','먼저','이유','역시','했다','코로나','가능성이','않다','되어','돈을','위해서는','수는','당장','보고서','표''닫기','개요','가지','아예','필요가','있다고','것도','시간이','아니다','아직','현재의','사실상','경우가','차이가','바로','된다','합의사항','이제','갈수록','것에','현','결국','건','발표한','않는다','않고','국회','대통령','콘텐츠','사용','방송','프로그램','기사','시간','서울','문단','이전','기타','영상','현재','자체가','자신의','점점','같다','못하는','받는','수밖에','나오는','양립','국제','인용','위','인권','개선','된','채용','하락','강화','시작','질','활동','기본','분야','대폭','주거','억','통한','전국','연합뉴스','선택','조','의견','성질','상품','도입','우리나라의','뉴스','방안','이미지가','대응','윤석열','추진','네이버','유튜브','댓글','논문','위원회','길','기본계획','보도자료','뉴스레터','공지사항','더보기','극복','공유','과제','글로벌','통권','호','그림','본문','바로가기','끝','회원가입','이용안내','기사를','추천','키워드','공유하기','영역','기자','기사본문','카카오톡','이메일','광고','이슈','구독','문의','이번','전체메뉴','팝업','없습니다','배너','있어요','참고문헌','리뷰','인증','가능합니다','기관','신청','싸이티지','모달','버튼','사진','레이어','이미지','하단','상단','우측','포토','동일','조회','편집패널','기사면','댓글삭제','기사보내기','오피니언','전체기사','비밀번호','제목','리스트','윤','이름','연합인증','라벨','종이책','새창이동','의견을','대한민국의','내용은','인식','인식이','서울시','지자체','소속','낳을','평균','한국에서','두고','낳아', '하면', '나온다', '그나마', '비슷한', '좀', '인구의', '한국', '만명', '국가', '되지', '일본', '한국의', '키우는', '정부의', '등', '선진국', '대한민국', '주는', '년대에', '국민', '문제에', '외국인', '청년', '부담이', '부동산', '그래도', '최고', '가구', '사회', '차', '국가들의', '예정이다', '여성의', '지속적인', '경우에는', '향후', '도서', '전망이다', '역사', '관점', '대책', '표', '홍콩', '기대', '기능', '닫기', '해야', '문제가', '심각한', '문화', '하위', '관계', '에서', '경기', '정부는', '마련', '집중', '적극', '내국인', '문제는', '대비', '중위', '의한', '국내', '만에', '세대', '과도한', '위하여', '판', '교통', '해외', '정치', '국무회의', '공공누리', '산업', '기업', '만원', '인물', '사건', '사회의', '노동자', '스포츠', '온라인', '영화', '통계청', '연구', '정책은', '젊은', '필요하다', '외', '역할을', '증가하고', '자주', '교수는', '개발', '받을', '신생아', '해결할', '정보를', '정보', '효과를', '예를', '원인으로', '핵심', '금지', '대표', '개인의', '생각', '조영태', '대학', '날', '효과가', '난임', '주간', '자동차', '오늘', '생각이', '기존', '한국보건사회연구원', '평가', '일자리', '관련기사', '변화에', '않습니다', '활성화', '정책에', '내용을', '참여', '우리나라는', '밝혔다', '부산', '대통령은', '시장', '내가', '사설', '제조', '고용', '관련된', '혜택을', '점에서', '말했다', '교수', '관련해서는', '대통령이', '투자', '보건복지부', '제', '시행', '제공', '부처', '대상', '사회가', '완화', '개월', '정책적', '확산', '우리가', '통화정책', '제고', '조성', '국방', '동아일보', '우리나라', '과학', '사업', '대책이', '최신', '블로그', '분석', '박스', '인사말', '구성', '위원', '오시는', '행사', '카드뉴스', '자료실', '정보공개', '장래인구추계', '주소', '서울특별시', '개최', '제호', '행정안전부', '기획', '운영', '종합', '미디어', '유형', '목록', '질문', '이용약관', '전자정부', '가구에', '입니다', '관리', '주세요', '한국인구학', '원인과', '입력', '설정', '단독', '국군의날', '임시공휴일', '의결', '이재명', '응급실', '공감', '연재', '칼럼', '오늘의', '최신기사', '국회정당', '금융', '사건사고', '실시간', '복사', '공개', '삭제', '고객센터', '검색창', '분', '좋아요', '창', '프린트', '페이스북', '자동', '재배포', '컨텐츠', '기사뷰', '요약', '동영상', '이시다', '공통', '라디오', '원문', '확인', '곳서', '딥페이크', '전체보기', '새', '계약', '중앙일보', '공감수', '정성호', '한국투자증권', '완료', '회사소개', '알림', '홈페이지', '기능을', '회', '됩니다', '제보', '페이지', '스크롤', '열기', '카카오', '트위터', '신고', '계획이다', '섹션', '홈', '대책은', '활용', '피플', '논문을', '소속기관', '불가합니다', '종료', '서지관리', '밀리의', '서재', '쿠폰', '무료', '검색어', '이용된', '누리집', '인스타그램', '지역본부', '금융안정', '경제교육', '민원', '민원신청', '업무보고', '구축', '단계', '한국사회', '텍스트', '확대하고', '분야별', '업무추진비', '공공데이터', '적극행정', '주요정책', '가족친화적', '한줄평', '판매지수', '추진할', '지방자치단체는', '개정', '신설', '대응을', '주요업무', '여부', '폰트', '매일일보','비롯한','프랑스의','이제는','개의','국가들은','매년','일본의','부모의','있었다','국민의','정책의','정책을','문제를','정부','변경','정책과','예상된다','뒤','월에','내년','추계','변화가','제주','세종','계획','문제','제도','위기','현황','우선','공무원','줄이기','쉬운','요인','사회에','일자리를','사회에서','해결','살','신규','지급','방법','디지털','극복을','전략','울산','저작권','책','글자크기','등록번호','연예','채널','전망된다','개인정보','논문의','모달팝업','세트','정책이','비율이','있어서','전망','추석','해','사람들','대책을','사이트맵','헤더','브랜드','국군의','선거','보니','목표','아래','중소기업','앞으로','경쟁이','예산을','근본적인','일과','지방','그래픽','개인','기자의','경제정책','밴드',]
# 여기에 제외할 단어들을 추가하세요

def get_search_results(query, num_results):
    # 구글에 원하는 단어로 검색 몇개 검색할지 정해줌 
    search_url = f"https://www.google.com/search?q={urllib.parse.quote(query)}&num={num_results}"
    # 봇이 아닌 것처럼 만들어줌
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
    }
    try:
        # get요청을 보냄
        response = requests.get(search_url, headers=headers)
        # 요청 실패시 404에러
        response.raise_for_status()
        # text데이터 파싱 BeautifulSoup객체
        soup = BeautifulSoup(response.text, 'html.parser')
        # 주소들 넣은 list 빈공간
        links = []
        # a태크에 주소 있는 부분 모두 가져와서 하나하나 순서대로  a_tag로 주면서 for문처럼하나씩 작업
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            if 'url=' in href:
                # 주소 전처리 
                    # url= ~ & 사이에 있는 문자열 
                link = href.split('url=')[1].split('&')[0]
                # 주소들 필터링
                if not ('.jpg' in link or '.png' in link or '.gif' in link or '.pdf' in link or '.mp4' in link
                        or 'vimeo.com' in link or 'instagram.com' in link or 'imgur.com' in link or 'download' in link or 'attachment' in link or 'down.do' in link or 'FileDown.do' in link or 'google.com' in link or 'youtube.com' in link) and ('http' in link or 'https' in link):
                    # 디코딩 
                    links.append(urllib.parse.unquote(link))
        return links
    except requests.RequestException as e:
        print(f"HTTP 요청 중 오류 발생: {e}")
        return []

# 태그에 하위에 있는 문자열만 가져오게 만듬 중복 때문에
def extract_text_from_tag(tag):
    text = ''
    # tag안에 자료 모두 한번 씩 element로 넣음
    for element in tag.contents:
        # isinstance - bs4의 함수 -> element안에 str만 찾음 
        if isinstance(element, str):
            #element str만 빼고 다 무시(strip)
            text += element.strip() + ' '
    # 한번더 공백이 존재할 수 있으니까 strip()
    return text.strip()

def crawl_article(url):
    try:
        # get요청인데 8초동안 처리 못하면 넘어감
        response = requests.get(url, timeout=8)
        # 에러 처리
        response.raise_for_status()
        # 파싱
        soup = BeautifulSoup(response.text, 'html.parser')
        article_content = ''
        # 데이터 가져올 태그들
        tags_to_extract = [
            'p', 'div', 'article', 'section', 'header', 'footer', 'nav', 'aside',
            'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'blockquote', 'pre', 'code',
            'li', 'a', 'summary', 'span', 'strong', 'td'
        ]
        # tags_to_extract 태그들 tag_name으로 순차적으로 대입
        for tag_name in tags_to_extract:
            # soup 가져온 데이터에서 맞는 태그를 찾음 그리고 tag에 넣음
            for tag in soup.find_all(tag_name):
                # extract_text_from_tag(tag)실행해서 결과를 article_content에 저장 
                article_content += extract_text_from_tag(tag) + ' '
        return article_content
    except requests.RequestException as e:
        print(f"HTTP 요청 중 오류 발생: {e}")
        return None

# 단어 별로 모으기 위해서 
def preprocess_text(text):
    # 혹시 모르는 영어 소문자로 변환
    text = text.lower()
    # 한글과 공백을 제외한 모든 문자를 제거
    text = re.sub(r'[^가-힣\s]', '', text)
    # 연속된 공백을 단일 공백으로 변환
    text = re.sub(r'\s+', ' ', text)
    return text


def extract_words(text):
    # 공백 한번 더 제거 전처리과정에서 생기는 공백을 제거하기 위해서 
    words = text.split()
    return words


def analyze_content(content):
    preprocessed_text = preprocess_text(content)
    words = extract_words(preprocessed_text)
    # Counter 단어별로 몇번 나왔는지 카운트 
    word_counts = Counter(words)
    return word_counts

# word_counts단어들 , min_count 최소 갯수 , exclude_words제외할 단어
def filter_words(word_counts, min_count, exclude_words):
    # 제외할 단어 리스트를 포함하여 필터링
    # min_count보다 큰경우만 찾아, word 안에 exclude_words이 단어들이 없어야 한다.는 조건을 걸려있음
    # word-키, count-값 으로 저장한 딕셔너리로 저장 
    filtered_counts = {word: count for word, count in word_counts.items() if count >= min_count and word not in exclude_words}
    return filtered_counts

# 외부에서 실행되지 않게 설정
if __name__ == "__main__":
    query = "저출산"
    # 저출산을 단어로 50개 검색
    search_results = get_search_results(query, 50)
    # 전체 단어 빈도수 
    total_word_counts = Counter()
    
    if search_results:
        for url in search_results:
            # 크롤링한 url표기
            print(f"\n크롤링 중인 URL: {url}")
            # 캐그별로 분류된 데이터
            content = crawl_article(url)
            if content:
                # 단어 카운트
                word_counts = analyze_content(content)
                # 
                total_word_counts.update(word_counts)
            else:
                print("크롤링된 내용이 없습니다.")
            time.sleep(0.5)
    
    # 다 하고 나서 원하는 데이터만 가져오게 설정 
    # 빈도수가 20번 이상인 것들만 가져오고 필터링
    min_count = 20
    filtered_word_counts = filter_words(total_word_counts, min_count, EXCLUDE_WORDS)
    
    print(f"\n{query}에 관한 총 단어 발생 횟수 ({min_count}회 이상):")
    # .items() 모든 키-값 쌍 word-키, count-값 으로 할당
    for word, count in filtered_word_counts.items():
        print(f"'{word}': {count}")
    
